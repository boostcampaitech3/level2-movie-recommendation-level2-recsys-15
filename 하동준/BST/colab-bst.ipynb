{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vertical-somewhere",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Notebooks is a join notebook from both the prepare_data and pytorch-bst in order to be run in google colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-nutrition",
   "metadata": {},
   "source": [
    "# Prepare data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indirect-chapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "import math\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-constraint",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-carbon",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "medical-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n",
    "ZipFile(\"movielens.zip\", \"r\").extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "iraqi-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99190/4154562035.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  users = pd.read_csv(\n",
      "/tmp/ipykernel_99190/4154562035.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(\n",
      "/tmp/ipykernel_99190/4154562035.py:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movies = pd.read_csv(\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 3114: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m users \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml-1m/users.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccupation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip_code\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m ratings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml-1m/ratings.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munix_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m movies \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mml-1m/movies.dat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m::\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1231\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:115\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    110\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[1;32m    111\u001b[0m (\n\u001b[1;32m    112\u001b[0m     columns,\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[0;32m--> 115\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns: \u001b[38;5;28mlist\u001b[39m[Hashable]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:501\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffered_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m names:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:596\u001b[0m, in \u001b[0;36mPythonParser._buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:696\u001b[0m, in \u001b[0;36mPythonParser._next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m     orig_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m orig_line \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:760\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:221\u001b[0m, in \u001b[0;36mPythonParser._make_reader.<locals>._read\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read\u001b[39m():\n\u001b[0;32m--> 221\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     pat \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(sep)\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pat\u001b[38;5;241m.\u001b[39msplit(line\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3114: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv(\n",
    "    \"ml-1m/users.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    \"ml-1m/movies.dat\", sep=\"::\", names=[\"movie_id\", \"title\", \"genres\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legendary-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Movies\n",
    "movies[\"year\"] = movies[\"title\"].apply(lambda x: x[-5:-1])\n",
    "movies.year = pd.Categorical(movies.year)\n",
    "movies[\"year\"] = movies.year.cat.codes\n",
    "## Users\n",
    "users.sex = pd.Categorical(users.sex)\n",
    "users[\"sex\"] = users.sex.cat.codes\n",
    "\n",
    "\n",
    "users.age_group = pd.Categorical(users.age_group)\n",
    "users[\"age_group\"] = users.age_group.cat.codes\n",
    "\n",
    "\n",
    "users.occupation = pd.Categorical(users.occupation)\n",
    "users[\"occupation\"] = users.occupation.cat.codes\n",
    "\n",
    "\n",
    "users.zip_code = pd.Categorical(users.zip_code)\n",
    "users[\"zip_code\"] = users.zip_code.cat.codes\n",
    "\n",
    "#Ratings\n",
    "ratings['unix_timestamp'] = pd.to_datetime(ratings['unix_timestamp'],unit='s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "voluntary-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save primary csv's\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "    \n",
    "    \n",
    "users.to_csv(\"data/users.csv\",index=False)\n",
    "movies.to_csv(\"data/movies.csv\",index=False)\n",
    "ratings.to_csv(\"data/ratings.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artificial-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Movies\n",
    "movies[\"movie_id\"] = movies[\"movie_id\"].astype(str)\n",
    "## Users\n",
    "users[\"user_id\"] = users[\"user_id\"].astype(str)\n",
    "\n",
    "##Ratings \n",
    "ratings[\"movie_id\"] = ratings[\"movie_id\"].astype(str)\n",
    "ratings[\"user_id\"] = ratings[\"user_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animated-slovenia",
   "metadata": {
    "id": "6_CC3yYCLxVN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m genres \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdventure\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWestern\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m genre \u001b[38;5;129;01min\u001b[39;00m genres:\n\u001b[0;32m---> 23\u001b[0m     movies[genre] \u001b[38;5;241m=\u001b[39m \u001b[43mmovies\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m values: \u001b[38;5;28mint\u001b[39m(genre \u001b[38;5;129;01min\u001b[39;00m values\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     25\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "genres = [\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children's\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]\n",
    "\n",
    "for genre in genres:\n",
    "    movies[genre] = movies[\"genres\"].apply(\n",
    "        lambda values: int(genre in values.split(\"|\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-calgary",
   "metadata": {
    "id": "0KsqW_4rLxVN"
   },
   "source": [
    "### Transform the movie ratings data into sequences\n",
    "\n",
    "First, let's sort the the ratings data using the `unix_timestamp`, and then group the\n",
    "`movie_id` values and the `rating` values by `user_id`.\n",
    "\n",
    "The output DataFrame will have a record for each `user_id`, with two ordered lists\n",
    "(sorted by rating datetime): the movies they have rated, and their ratings of these movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "light-publicity",
   "metadata": {
    "id": "D5v700zTLxVN"
   },
   "outputs": [],
   "source": [
    "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
    "\n",
    "ratings_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(ratings_group.groups.keys()),\n",
    "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
    "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
    "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-affair",
   "metadata": {
    "id": "USa6rk0eLxVN"
   },
   "source": [
    "Now, let's split the `movie_ids` list into a set of sequences of a fixed length.\n",
    "We do the same for the `ratings`. Set the `sequence_length` variable to change the length\n",
    "of the input sequence to the model. You can also change the `step_size` to control the\n",
    "number of sequences to generate for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "boxed-chick",
   "metadata": {
    "id": "XdhRJlxULxVN"
   },
   "outputs": [],
   "source": [
    "sequence_length = 8\n",
    "step_size = 1\n",
    "\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "\n",
    "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.ratings = ratings_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del ratings_data[\"timestamps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-catalyst",
   "metadata": {
    "id": "5dYEduaqLxVN"
   },
   "source": [
    "After that, we process the output to have each sequence in a separate records in\n",
    "the DataFrame. In addition, we join the user features with the ratings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "visible-tuner",
   "metadata": {
    "id": "gM5_RBACLxVO"
   },
   "outputs": [],
   "source": [
    "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
    "    \"movie_ids\", ignore_index=True\n",
    ")\n",
    "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
    "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
    "ratings_data_transformed = ratings_data_transformed.join(\n",
    "    users.set_index(\"user_id\"), on=\"user_id\"\n",
    ")\n",
    "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n",
    "    lambda x: \",\".join(x)\n",
    ")\n",
    "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
    "    lambda x: \",\".join([str(v) for v in x])\n",
    ")\n",
    "\n",
    "del ratings_data_transformed[\"zip_code\"]\n",
    "\n",
    "ratings_data_transformed.rename(\n",
    "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-relief",
   "metadata": {
    "id": "YnxbQ-wQLxVO"
   },
   "source": [
    "With `sequence_length` of 4 and `step_size` of 2, we end up with 498,623 sequences.\n",
    "\n",
    "Finally, we split the data into training and testing splits, with 85% and 15% of\n",
    "the instances, respectively, and store them to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subsequent-thanks",
   "metadata": {
    "id": "0lPMjBoRLxVO"
   },
   "outputs": [],
   "source": [
    "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
    "train_data = ratings_data_transformed[random_selection]\n",
    "test_data = ratings_data_transformed[~random_selection]\n",
    "\n",
    "train_data.to_csv(\"data/train_data.csv\", index=False, sep=\",\")\n",
    "test_data.to_csv(\"data/test_data.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d346a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prerequisite-plymouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_movie_ids</th>\n",
       "      <th>sequence_ratings</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1022,2340,1836,3408,1207,2804,260,720</td>\n",
       "      <td>5,3,5,4,4,5,4,3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2028,3105,938,1035,1962,1028,2018,150</td>\n",
       "      <td>5,5,4,5,4,5,4,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1097,914,1287,2797,1246,2762,661,2918</td>\n",
       "      <td>4,3,5,4,4,4,3,4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2797,1246,2762,661,2918,531,3114,2791</td>\n",
       "      <td>4,4,4,3,4,4,4,4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>2398,1545,527,745,595,588,1,2687</td>\n",
       "      <td>4,4,5,3,5,4,5,3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963917</th>\n",
       "      <td>999</td>\n",
       "      <td>3409,2120,86,481,31,1589,3791,225</td>\n",
       "      <td>4,3,3,4,3,4,4,4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963939</th>\n",
       "      <td>999</td>\n",
       "      <td>1027,1442,2146,358,2541,2431,1727,3100</td>\n",
       "      <td>4,4,3,4,3,4,3,4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963943</th>\n",
       "      <td>999</td>\n",
       "      <td>2541,2431,1727,3100,2320,996,3513,3109</td>\n",
       "      <td>3,4,3,4,3,2,3,3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963948</th>\n",
       "      <td>999</td>\n",
       "      <td>996,3513,3109,3156,282,1888,209,207</td>\n",
       "      <td>2,3,3,4,4,3,2,4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963956</th>\n",
       "      <td>999</td>\n",
       "      <td>79,2875,2316,2165,361,2688,24,2264</td>\n",
       "      <td>3,4,3,1,3,3,3,2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145019 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                      sequence_movie_ids sequence_ratings  sex  \\\n",
       "3            1   1022,2340,1836,3408,1207,2804,260,720  5,3,5,4,4,5,4,3    0   \n",
       "16           1   2028,3105,938,1035,1962,1028,2018,150  5,5,4,5,4,5,4,5    0   \n",
       "24           1   1097,914,1287,2797,1246,2762,661,2918  4,3,5,4,4,4,3,4    0   \n",
       "27           1   2797,1246,2762,661,2918,531,3114,2791  4,4,4,3,4,4,4,4    0   \n",
       "39           1        2398,1545,527,745,595,588,1,2687  4,4,5,3,5,4,5,3    0   \n",
       "...        ...                                     ...              ...  ...   \n",
       "963917     999       3409,2120,86,481,31,1589,3791,225  4,3,3,4,3,4,4,4    1   \n",
       "963939     999  1027,1442,2146,358,2541,2431,1727,3100  4,4,3,4,3,4,3,4    1   \n",
       "963943     999  2541,2431,1727,3100,2320,996,3513,3109  3,4,3,4,3,2,3,3    1   \n",
       "963948     999     996,3513,3109,3156,282,1888,209,207  2,3,3,4,4,3,2,4    1   \n",
       "963956     999      79,2875,2316,2165,361,2688,24,2264  3,4,3,1,3,3,3,2    1   \n",
       "\n",
       "        age_group  occupation  \n",
       "3               0          10  \n",
       "16              0          10  \n",
       "24              0          10  \n",
       "27              0          10  \n",
       "39              0          10  \n",
       "...           ...         ...  \n",
       "963917          2          15  \n",
       "963939          2          15  \n",
       "963943          2          15  \n",
       "963948          2          15  \n",
       "963956          2          15  \n",
       "\n",
       "[145019 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-brother",
   "metadata": {},
   "source": [
    "# BST Implementation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "import math\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "needed-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\n",
    "    \"data/users.csv\",\n",
    "    sep=\",\",\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    \"data/ratings.csv\",\n",
    "    sep=\",\",\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    \"data/movies.csv\", sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-responsibility",
   "metadata": {},
   "source": [
    "## Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confused-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MovieDataset(data.Dataset):\n",
    "    \"\"\"Movie dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, ratings_file,test=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with user,past,future.\n",
    "        \"\"\"\n",
    "        self.ratings_frame = pd.read_csv(\n",
    "            ratings_file,\n",
    "            delimiter=\",\",\n",
    "            # iterator=True,\n",
    "        )\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.ratings_frame.iloc[idx]\n",
    "        user_id = data.user_id\n",
    "        \n",
    "        movie_history = eval(data.sequence_movie_ids)\n",
    "        movie_history_ratings = eval(data.sequence_ratings)\n",
    "        target_movie_id = movie_history[-1:][0]\n",
    "        target_movie_rating = movie_history_ratings[-1:][0]\n",
    "        \n",
    "        movie_history = torch.LongTensor(movie_history[:-1])\n",
    "        movie_history_ratings = torch.LongTensor(movie_history_ratings[:-1])\n",
    "\n",
    "        \n",
    "        \n",
    "        sex = data.sex\n",
    "        age_group = data.age_group\n",
    "        occupation = data.occupation\n",
    "        \n",
    "        return user_id, movie_history, target_movie_id,  movie_history_ratings, target_movie_rating, sex, age_group, occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "difficult-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children's\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]\n",
    "\n",
    "for genre in genres:\n",
    "    movies[genre] = movies[\"genres\"].apply(\n",
    "        lambda values: int(genre in values.split(\"|\"))\n",
    "    )\n",
    "    \n",
    "sequence_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                       | Type                    | Params\n",
      "------------------------------------------------------------------------\n",
      "0  | embeddings_user_id         | Embedding               | 471 K \n",
      "1  | embeddings_user_sex        | Embedding               | 2     \n",
      "2  | embeddings_age_group       | Embedding               | 14    \n",
      "3  | embeddings_user_occupation | Embedding               | 84    \n",
      "4  | embeddings_user_zip_code   | Embedding               | 3.4 K \n",
      "5  | embeddings_movie_id        | Embedding               | 249 K \n",
      "6  | embeddings_position        | Embedding               | 504   \n",
      "7  | embeddings_movie_genre     | Embedding               | 69.9 K\n",
      "8  | embeddings_movie_year      | Embedding               | 729   \n",
      "9  | transfomerlayer            | TransformerEncoderLayer | 276 K \n",
      "10 | linear                     | Sequential              | 1.3 M \n",
      "11 | criterion                  | MSELoss                 | 0     \n",
      "12 | mae                        | MeanAbsoluteError       | 0     \n",
      "13 | mse                        | MeanSquaredError        | 0     \n",
      "------------------------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "69.9 K    Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.328     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161bdefd60d846a2b0d31b9b9a7636d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BST(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, args=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        super(BST, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.args = args\n",
    "        #-------------------\n",
    "        # Embedding layers\n",
    "        ##Users \n",
    "        self.embeddings_user_id = nn.Embedding(\n",
    "            int(users.user_id.max())+1, int(math.sqrt(users.user_id.max()))+1\n",
    "        )\n",
    "        ###Users features embeddings\n",
    "        self.embeddings_user_sex = nn.Embedding(\n",
    "            len(users.sex.unique()), int(math.sqrt(len(users.sex.unique())))\n",
    "        )\n",
    "        self.embeddings_age_group = nn.Embedding(\n",
    "            len(users.age_group.unique()), int(math.sqrt(len(users.age_group.unique())))\n",
    "        )\n",
    "        self.embeddings_user_occupation = nn.Embedding(\n",
    "            len(users.occupation.unique()), int(math.sqrt(len(users.occupation.unique())))\n",
    "        )\n",
    "        self.embeddings_user_zip_code = nn.Embedding(\n",
    "            len(users.zip_code.unique()), int(math.sqrt(len(users.sex.unique())))\n",
    "        )\n",
    "        \n",
    "        ##Movies\n",
    "        self.embeddings_movie_id = nn.Embedding(\n",
    "            int(movies.movie_id.max())+1, int(math.sqrt(movies.movie_id.max()))+1\n",
    "        )\n",
    "        self.embeddings_position  = nn.Embedding(\n",
    "           sequence_length, int(math.sqrt(len(movies.movie_id.unique())))+1\n",
    "        )\n",
    "        ###Movies features embeddings\n",
    "        genre_vectors = movies[genres].to_numpy()\n",
    "        self.embeddings_movie_genre = nn.Embedding(\n",
    "            genre_vectors.shape[0], genre_vectors.shape[1]\n",
    "        )\n",
    "        \n",
    "        self.embeddings_movie_genre.weight.requires_grad = False #Not training genres\n",
    "        \n",
    "        \n",
    "        self.embeddings_movie_year = nn.Embedding(\n",
    "            len(movies.year.unique()), int(math.sqrt(len(movies.year.unique())))\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Network\n",
    "        self.transfomerlayer = nn.TransformerEncoderLayer(63, 3, dropout=0.2)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                589,\n",
    "                1024,\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.mse = torchmetrics.MeanSquaredError()\n",
    "        \n",
    "    def encode_input(self,inputs):\n",
    "        user_id, movie_history, target_movie_id,  movie_history_ratings, target_movie_rating, sex, age_group, occupation = inputs\n",
    "        \n",
    "        \n",
    "        #MOVIES\n",
    "        movie_history = self.embeddings_movie_id(movie_history)\n",
    "        target_movie = self.embeddings_movie_id(target_movie_id)\n",
    "        \n",
    "        positions = torch.arange(0,sequence_length-1,1,dtype=int,device=self.device)\n",
    "        positions = self.embeddings_position(positions)\n",
    "        \n",
    "        encoded_sequence_movies_with_poistion_and_rating = (movie_history + positions) #Yet to multiply by rating\n",
    "        \n",
    "        target_movie = torch.unsqueeze(target_movie, 1)\n",
    "        transfomer_features = torch.cat((encoded_sequence_movies_with_poistion_and_rating, target_movie),dim=1)\n",
    "        \n",
    "        #USERS\n",
    "        user_id = self.embeddings_user_id(user_id)\n",
    "        \n",
    "        sex = self.embeddings_user_sex(sex)\n",
    "        age_group = self.embeddings_age_group(age_group)\n",
    "        occupation = self.embeddings_user_occupation(occupation)\n",
    "        user_features = torch.cat((user_id, sex, age_group,occupation), 1)\n",
    "        \n",
    "        return transfomer_features, user_features, target_movie_rating.float()\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        transfomer_features, user_features, target_movie_rating = self.encode_input(batch)\n",
    "        transformer_output = self.transfomerlayer(transfomer_features)\n",
    "        transformer_output = torch.flatten(transformer_output,start_dim=1)\n",
    "        \n",
    "        #Concat with other features\n",
    "        features = torch.cat((transformer_output,user_features),dim=1)\n",
    "        \n",
    "        output = self.linear(features)\n",
    "        return output, target_movie_rating\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out, target_movie_rating = self(batch)\n",
    "        out = out.flatten()\n",
    "        loss = self.criterion(out, target_movie_rating)\n",
    "        \n",
    "        mae = self.mae(out, target_movie_rating)\n",
    "        mse = self.mse(out, target_movie_rating)\n",
    "        rmse =torch.sqrt(mse)\n",
    "        self.log(\n",
    "            \"train/mae\", mae, on_step=True, on_epoch=False, prog_bar=False\n",
    "        )\n",
    "        \n",
    "        self.log(\n",
    "            \"train/rmse\", rmse, on_step=True, on_epoch=False, prog_bar=False\n",
    "        )\n",
    "        \n",
    "        self.log(\"train/step_loss\", loss, on_step=True, on_epoch=False, prog_bar=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out, target_movie_rating = self(batch)\n",
    "        out = out.flatten()\n",
    "        loss = self.criterion(out, target_movie_rating)\n",
    "        \n",
    "        mae = self.mae(out, target_movie_rating)\n",
    "        mse = self.mse(out, target_movie_rating)\n",
    "        rmse =torch.sqrt(mse)\n",
    "        \n",
    "        return {\"val_loss\": loss, \"mae\": mae.detach(), \"rmse\":rmse.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_mae = torch.stack([x[\"mae\"] for x in outputs]).mean()\n",
    "        avg_rmse = torch.stack([x[\"rmse\"] for x in outputs]).mean()\n",
    "        \n",
    "        self.log(\"val/loss\", avg_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/mae\", avg_mae, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/rmse\", avg_rmse, on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        users = torch.cat([x[\"users\"] for x in outputs])\n",
    "        y_hat = torch.cat([x[\"top14\"] for x in outputs])\n",
    "        users = users.tolist()\n",
    "        y_hat = y_hat.tolist()\n",
    "        \n",
    "        data = {\"users\": users, \"top14\": y_hat}\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        print(len(df))\n",
    "        df.to_csv(\"lightning_logs/predict.csv\", index=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.0005)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "        return parser\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print(\"Loading datasets\")\n",
    "        self.train_dataset = MovieDataset(\"data/train_data.csv\")\n",
    "        self.val_dataset = MovieDataset(\"data/test_data.csv\")\n",
    "        self.test_dataset = MovieDataset(\"data/test_data.csv\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "        \n",
    "model = BST()\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-split",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "CUDAtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
