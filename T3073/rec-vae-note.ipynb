{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecVAE\n",
    "\n",
    "* [Official Repository](https://github.com/ilya-shenbin/RecVAE)\n",
    "* special mission에 올라온 Mult-VAE와 비슷하기 때문에 확인하시면 도움이 많이 됩니다.\n",
    "* 아래 모든 코드는 Official Repository에서 가져왔습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 - preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dataset: train-ratings.csv 위치 (변경 필요)\n",
    "    * column이 userId, movieId, rating으로 되어 있어야함\n",
    "* output_dir: 모델 학습에 필요한 csv, txt 파일이 생성되는 위치 (변경 필요)\n",
    "* save: 모델 파라미터 저장을 위한 파일 이름 (변경 필요)\n",
    "* n-epochs: epoch 수 (변경 필요)\n",
    "* heldout_users: val, test에 사용할 유저 수(Mult-VAE와 동일하게 3000으로 유지)\n",
    "* lr: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset',default='/opt/ml/input/RecVAE/rec-vae-train-ratings.csv',type=str)\n",
    "parser.add_argument('--output_dir', type=str, default='/opt/ml/input/RecVAE/output_dir')\n",
    "parser.add_argument('--threshold', type=float,default=0)\n",
    "parser.add_argument('--min_items_per_user', type=int, default=5)\n",
    "parser.add_argument('--min_users_per_item', type=int, default=0)\n",
    "parser.add_argument('--heldout_users',default=3000, type=int)\n",
    "parser.add_argument('--save', type=str, default='recvae-500-0.0001-test.pt',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "parser.add_argument('--hidden-dim', type=int, default=600)\n",
    "parser.add_argument('--latent-dim', type=int, default=200)\n",
    "parser.add_argument('--batch-size', type=int, default=500)\n",
    "parser.add_argument('--beta', type=float, default=None)\n",
    "parser.add_argument('--gamma', type=float, default=0.005)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--n-epochs', type=int, default=50)\n",
    "parser.add_argument('--n-enc_epochs', type=int, default=3)\n",
    "parser.add_argument('--n-dec_epochs', type=int, default=1)\n",
    "parser.add_argument('--not-alternating', type=bool, default=False)\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4643</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0      11     4643       1\n",
       "1      11      170       1\n",
       "2      11      531       1\n",
       "3      11      616       1\n",
       "4      11     2140       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = args.dataset\n",
    "output_dir = args.output_dir\n",
    "threshold = args.threshold\n",
    "min_uc = args.min_items_per_user\n",
    "min_sc = args.min_users_per_item\n",
    "n_heldout_users = args.heldout_users\n",
    "\n",
    "raw_data = pd.read_csv(dataset, header=0)\n",
    "raw_data = raw_data[raw_data['rating'] > threshold]\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "\n",
    "\n",
    "def filter_triplets(tp, min_uc=min_uc, min_sc=min_sc): \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 5154471 watching events from 31360 users and 6807 movies (sparsity: 2.415%)\n"
     ]
    }
   ],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)\n",
    "\n",
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))\n",
    "\n",
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "\n",
    "n_users = unique_uid.size\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
    "\n",
    "unique_sid = pd.unique(train_plays['movieId'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(os.path.join(output_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "        \n",
    "with open(os.path.join(output_dir, 'unique_uid.txt'), 'w') as f:\n",
    "    for uid in unique_uid:\n",
    "        f.write('%s\\n' % uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te\n",
    "\n",
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
    "\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
    "\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "\n",
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(output_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(output_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(output_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(output_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 - run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "        super(CompositePrior, self).__init__()\n",
    "        \n",
    "        self.mixture_weights = mixture_weights\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "        \n",
    "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.encoder_old.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
    "        \n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "        \n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "        \n",
    "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "                \n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "    \n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "        \n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
    "    \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
    "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)    \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred = self.decoder(z)\n",
    "        \n",
    "        if calculate_loss:\n",
    "            if gamma:\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                kl_weight = gamma * norm\n",
    "            elif beta:\n",
    "                kl_weight = beta\n",
    "\n",
    "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "            negative_elbo = -(mll - kld)\n",
    "            \n",
    "            return (mll, kld), negative_elbo\n",
    "            \n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file, n_items, n_users, global_indexing=False):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    \n",
    "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_tr_te_data(csv_file_tr, csv_file_te, n_items, n_users, global_indexing=False):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    if global_indexing:\n",
    "        start_idx = 0\n",
    "        end_idx = len(unique_uid) - 1\n",
    "    else:\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te\n",
    "\n",
    "def get_data(dataset, global_indexing=False):\n",
    "    unique_sid = list()\n",
    "    with open(os.path.join(dataset, 'unique_sid.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            unique_sid.append(line.strip())\n",
    "    \n",
    "    unique_uid = list()\n",
    "    with open(os.path.join(dataset, 'unique_uid.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            unique_uid.append(line.strip())\n",
    "            \n",
    "    n_items = len(unique_sid)\n",
    "    n_users = len(unique_uid)\n",
    "    \n",
    "    train_data = load_train_data(os.path.join(dataset, 'train.csv'), n_items, n_users, global_indexing=global_indexing)\n",
    "\n",
    "\n",
    "    vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(dataset, 'validation_tr.csv'),\n",
    "                                               os.path.join(dataset, 'validation_te.csv'),\n",
    "                                               n_items, n_users, \n",
    "                                               global_indexing=global_indexing)\n",
    "\n",
    "    test_data_tr, test_data_te = load_tr_te_data(os.path.join(dataset, 'test_tr.csv'),\n",
    "                                                 os.path.join(dataset, 'test_te.csv'),\n",
    "                                                 n_items, n_users, \n",
    "                                                 global_indexing=global_indexing)\n",
    "    \n",
    "    data = train_data, vad_data_tr, vad_data_te, test_data_tr, test_data_te\n",
    "    data = (x.astype('float32') for x in data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def ndcg(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def recall(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "data = get_data(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_in_data, valid_out_data, test_in_data, test_out_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "        \n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)\n",
    "\n",
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "    metrics = deepcopy(metrics)\n",
    "    model.eval()\n",
    "    \n",
    "    for m in metrics:\n",
    "        m['score'] = []\n",
    "    \n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "        \n",
    "        if not (data_in is data_out):\n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "            \n",
    "        for m in metrics:\n",
    "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = np.concatenate(m['score']).mean()\n",
    "        \n",
    "    return [x['score'] for x in metrics]\n",
    "\n",
    "\n",
    "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
    "            loss.backward()\n",
    "            \n",
    "            for optimizer in opts:\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'hidden_dim': args.hidden_dim,\n",
    "    'latent_dim': args.latent_dim,\n",
    "    'input_dim': train_data.shape[1]\n",
    "}\n",
    "metrics = [{'metric': ndcg, 'k': 100}]\n",
    "\n",
    "best_ndcg = -np.inf\n",
    "train_scores, valid_scores = [], []\n",
    "\n",
    "model = VAE(**model_kwargs).to(device)\n",
    "model_best = VAE(**model_kwargs).to(device)\n",
    "\n",
    "learning_kwargs = {\n",
    "    'model': model,\n",
    "    'train_data': train_data,\n",
    "    'batch_size': args.batch_size,\n",
    "    'beta': args.beta,\n",
    "    'gamma': args.gamma\n",
    "}\n",
    "\n",
    "decoder_params = set(model.decoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())\n",
    "\n",
    "optimizer_encoder = optim.Adam(encoder_params, lr=args.lr)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | valid ndcg@100: 0.1343 | best valid: 0.1343 | train ndcg@100: 0.2834\n",
      "epoch 1 | valid ndcg@100: 0.2373 | best valid: 0.2373 | train ndcg@100: 0.4340\n",
      "epoch 2 | valid ndcg@100: 0.3045 | best valid: 0.3045 | train ndcg@100: 0.5212\n",
      "epoch 3 | valid ndcg@100: 0.3295 | best valid: 0.3295 | train ndcg@100: 0.5576\n",
      "epoch 4 | valid ndcg@100: 0.3501 | best valid: 0.3501 | train ndcg@100: 0.5853\n",
      "epoch 5 | valid ndcg@100: 0.3645 | best valid: 0.3645 | train ndcg@100: 0.6076\n",
      "epoch 6 | valid ndcg@100: 0.3759 | best valid: 0.3759 | train ndcg@100: 0.6243\n",
      "epoch 7 | valid ndcg@100: 0.3838 | best valid: 0.3838 | train ndcg@100: 0.6368\n",
      "epoch 8 | valid ndcg@100: 0.3907 | best valid: 0.3907 | train ndcg@100: 0.6462\n",
      "epoch 9 | valid ndcg@100: 0.3957 | best valid: 0.3957 | train ndcg@100: 0.6512\n",
      "epoch 10 | valid ndcg@100: 0.4012 | best valid: 0.4012 | train ndcg@100: 0.6571\n",
      "epoch 11 | valid ndcg@100: 0.4052 | best valid: 0.4052 | train ndcg@100: 0.6623\n",
      "epoch 12 | valid ndcg@100: 0.4086 | best valid: 0.4086 | train ndcg@100: 0.6652\n",
      "epoch 13 | valid ndcg@100: 0.4113 | best valid: 0.4113 | train ndcg@100: 0.6695\n",
      "epoch 14 | valid ndcg@100: 0.4143 | best valid: 0.4143 | train ndcg@100: 0.6726\n",
      "epoch 15 | valid ndcg@100: 0.4160 | best valid: 0.4160 | train ndcg@100: 0.6759\n",
      "epoch 16 | valid ndcg@100: 0.4185 | best valid: 0.4185 | train ndcg@100: 0.6795\n",
      "epoch 17 | valid ndcg@100: 0.4206 | best valid: 0.4206 | train ndcg@100: 0.6818\n",
      "epoch 18 | valid ndcg@100: 0.4228 | best valid: 0.4228 | train ndcg@100: 0.6837\n",
      "epoch 19 | valid ndcg@100: 0.4238 | best valid: 0.4238 | train ndcg@100: 0.6860\n",
      "epoch 20 | valid ndcg@100: 0.4256 | best valid: 0.4256 | train ndcg@100: 0.6884\n",
      "epoch 21 | valid ndcg@100: 0.4271 | best valid: 0.4271 | train ndcg@100: 0.6900\n",
      "epoch 22 | valid ndcg@100: 0.4283 | best valid: 0.4283 | train ndcg@100: 0.6915\n",
      "epoch 23 | valid ndcg@100: 0.4299 | best valid: 0.4299 | train ndcg@100: 0.6919\n",
      "epoch 24 | valid ndcg@100: 0.4310 | best valid: 0.4310 | train ndcg@100: 0.6945\n",
      "epoch 25 | valid ndcg@100: 0.4322 | best valid: 0.4322 | train ndcg@100: 0.6952\n",
      "epoch 26 | valid ndcg@100: 0.4326 | best valid: 0.4326 | train ndcg@100: 0.6971\n",
      "epoch 27 | valid ndcg@100: 0.4341 | best valid: 0.4341 | train ndcg@100: 0.6978\n",
      "epoch 28 | valid ndcg@100: 0.4350 | best valid: 0.4350 | train ndcg@100: 0.6992\n",
      "epoch 29 | valid ndcg@100: 0.4365 | best valid: 0.4365 | train ndcg@100: 0.6995\n",
      "epoch 30 | valid ndcg@100: 0.4368 | best valid: 0.4368 | train ndcg@100: 0.7014\n",
      "epoch 31 | valid ndcg@100: 0.4378 | best valid: 0.4378 | train ndcg@100: 0.7025\n",
      "epoch 32 | valid ndcg@100: 0.4389 | best valid: 0.4389 | train ndcg@100: 0.7035\n",
      "epoch 33 | valid ndcg@100: 0.4393 | best valid: 0.4393 | train ndcg@100: 0.7039\n",
      "epoch 34 | valid ndcg@100: 0.4403 | best valid: 0.4403 | train ndcg@100: 0.7053\n",
      "epoch 35 | valid ndcg@100: 0.4407 | best valid: 0.4407 | train ndcg@100: 0.7063\n",
      "epoch 36 | valid ndcg@100: 0.4416 | best valid: 0.4416 | train ndcg@100: 0.7079\n",
      "epoch 37 | valid ndcg@100: 0.4415 | best valid: 0.4416 | train ndcg@100: 0.7076\n",
      "epoch 38 | valid ndcg@100: 0.4429 | best valid: 0.4429 | train ndcg@100: 0.7088\n",
      "epoch 39 | valid ndcg@100: 0.4439 | best valid: 0.4439 | train ndcg@100: 0.7086\n",
      "epoch 40 | valid ndcg@100: 0.4437 | best valid: 0.4439 | train ndcg@100: 0.7100\n",
      "epoch 41 | valid ndcg@100: 0.4443 | best valid: 0.4443 | train ndcg@100: 0.7109\n",
      "epoch 42 | valid ndcg@100: 0.4449 | best valid: 0.4449 | train ndcg@100: 0.7111\n",
      "epoch 43 | valid ndcg@100: 0.4453 | best valid: 0.4453 | train ndcg@100: 0.7119\n",
      "epoch 44 | valid ndcg@100: 0.4459 | best valid: 0.4459 | train ndcg@100: 0.7138\n",
      "epoch 45 | valid ndcg@100: 0.4468 | best valid: 0.4468 | train ndcg@100: 0.7133\n",
      "epoch 46 | valid ndcg@100: 0.4470 | best valid: 0.4470 | train ndcg@100: 0.7138\n",
      "epoch 47 | valid ndcg@100: 0.4475 | best valid: 0.4475 | train ndcg@100: 0.7136\n",
      "epoch 48 | valid ndcg@100: 0.4485 | best valid: 0.4485 | train ndcg@100: 0.7158\n",
      "epoch 49 | valid ndcg@100: 0.4490 | best valid: 0.4490 | train ndcg@100: 0.7156\n",
      "ndcg@100:\t0.4485\n",
      "recall@10:\t0.3860\n",
      "recall@50:\t0.4132\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    if args.not_alternating:\n",
    "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
    "    else:\n",
    "        run(opts=[optimizer_encoder], n_epochs=args.n_enc_epochs, dropout_rate=0.5, **learning_kwargs)\n",
    "        model.update_prior()\n",
    "        run(opts=[optimizer_decoder], n_epochs=args.n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
    "    \n",
    "    train_scores.append(\n",
    "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
    "    )\n",
    "\n",
    "    valid_scores.append(\n",
    "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
    "    )\n",
    "    \n",
    "    if valid_scores[-1] > best_ndcg:\n",
    "        best_ndcg = valid_scores[-1]\n",
    "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "        with open(args.save, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        \n",
    "\n",
    "    print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
    "          f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')\n",
    "\n",
    "\n",
    "    \n",
    "test_metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 10}, {'metric': recall, 'k': 50}]\n",
    "\n",
    "final_scores = evaluate(model_best, test_in_data, test_out_data, test_metrics)\n",
    "\n",
    "for metric, score in zip(test_metrics, final_scores):\n",
    "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('recvae-200-test.pt', 'rb') as f:\n",
    "#     model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_predict(model, data_in, data_out, samples_perc_per_epoch=1, batch_size=31360):\n",
    "    model.eval()\n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "    return ratings_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(csv_file, n_items, n_users, global_indexing=False):\n",
    "    tp = csv_file\n",
    "    \n",
    "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_pre = raw_data.copy()\n",
    "all_ = numerize(all_data_pre)\n",
    "all_.to_csv(os.path.join(args.output_dir,'all.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31360 6807\n"
     ]
    }
   ],
   "source": [
    "n_user, n_item = all_['uid'].nunique(), all_['sid'].nunique()\n",
    "\n",
    "print(n_user, n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_all_data(all_, n_item, n_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = all_predict(model_best, all_data, all_data,batch_size=n_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31360, 6807)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2527899 , -1.6820743 , -1.1181068 , ..., -7.7142687 ,\n",
       "        -4.924841  , -5.6800556 ],\n",
       "       [ 2.7746778 ,  3.4954822 ,  0.23952042, ..., -7.1501484 ,\n",
       "        -6.6979833 , -2.4345264 ],\n",
       "       [ 1.3404682 ,  1.5949605 ,  0.638039  , ..., -3.4893272 ,\n",
       "        -5.181338  , -2.748842  ],\n",
       "       ...,\n",
       "       [-0.53617734, -0.41650423, -1.2307526 , ..., -3.7785296 ,\n",
       "        -1.133735  , -3.4509425 ],\n",
       "       [-0.5175873 , -0.15063618, -0.15544726, ..., -2.6105168 ,\n",
       "        -0.4541959 , -2.96202   ],\n",
       "       [-0.77869034, -0.6500905 ,  3.5351746 , ..., -3.3811908 ,\n",
       "        -3.4420402 , -4.2058825 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5154471/5154471 [00:03<00:00, 1339218.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,j in tqdm(zip(all_['uid'],all_['sid']), total=len(all_)):\n",
    "    all_predictions[i,j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output.csv 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = np.argsort(all_predictions,axis=-1)[:,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 557, 1067,  299, ...,  421,  688,  692],\n",
       "       [ 295,  266,   40, ...,  467,  128,  111],\n",
       "       [1157,  572,  200, ...,  264,  570,   82],\n",
       "       ...,\n",
       "       [ 559, 1234,   78, ...,  375,  264, 1264],\n",
       "       [1584,  684,  948, ...,   46,  718, 1107],\n",
       "       [ 538,  629, 1073, ...,  419,  209, 2163]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2show = {v:k for k,v in show2id.items()}\n",
    "id2profile = {v:k for k,v in profile2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31360/31360 [00:00<00:00, 167185.52it/s]\n"
     ]
    }
   ],
   "source": [
    "users,items = [],[]\n",
    "for i in tqdm(range(len(top_10))):\n",
    "    users += [ id2profile[i] ] * 10\n",
    "    items += list(map(lambda x: id2show[x], top_10[i]))\n",
    "data_dict = {'user':users, 'item':items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([27968, 27968, 27968, 27968, 27968, 27968, 27968, 27968, 27968, 27968],\n",
       " [3949, 1086, 6016, 3362, 1222, 3435, 750, 1230, 1193, 1208])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['user'][:10], data_dict['item'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27968</td>\n",
       "      <td>3949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27968</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27968</td>\n",
       "      <td>6016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27968</td>\n",
       "      <td>3362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27968</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27968</td>\n",
       "      <td>3435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27968</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27968</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27968</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27968</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user  item\n",
       "0  27968  3949\n",
       "1  27968  1086\n",
       "2  27968  6016\n",
       "3  27968  3362\n",
       "4  27968  1222\n",
       "5  27968  3435\n",
       "6  27968   750\n",
       "7  27968  1230\n",
       "8  27968  1193\n",
       "9  27968  1208"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv.to_csv('output.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
